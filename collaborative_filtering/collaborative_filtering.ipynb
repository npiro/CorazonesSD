{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_name = 'train_ver2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import necessary stuff\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "\n",
    "processors = 4\n",
    "max_chunksize=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is my personal data folder. Change to your own\n",
    "#dir = \"/Users/piromast/Documents/Kaggle/SantanderData/\"\n",
    "import os\n",
    "dir = os.getcwd()\n",
    "#train_csv = os.path.join(dir,'train_ver2.csv')\n",
    "client_ids = np.load('../client_ids.npy')\n",
    "client_index_array = np.load('../' + csv_name + '__client_indices.npy')\n",
    "client_number = client_ids.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_chunks =  1365\n"
     ]
    }
   ],
   "source": [
    "import shlex\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "def get_dataset_lines():\n",
    "    cmd = \"wc -l \" + '../'+csv_name + '.csv'\n",
    "    process = Popen(shlex.split(cmd), stdout=PIPE)\n",
    "    (output, err) = process.communicate()\n",
    "    exit_code = process.wait\n",
    "    train_lines = int(output.split()[0])\n",
    "    return train_lines\n",
    "\n",
    "def get_train_reader():\n",
    "    return pd.read_csv(os.path.join('..',csv_name+'.csv'), chunksize=max_chunksize)\n",
    "\n",
    "#train_lines = get_dataset_lines()\n",
    "#print 'train_lines = ',train_lines\n",
    "\n",
    "train_chunks = int(np.ceil(float(train_lines)/chunksize))\n",
    "print 'train_chunks = ',train_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_reader = pd.read_csv(train_csv, chunksize=1000)\n",
    "#train_reader = pd.read_csv(train_csv, iterator=True, chunksize=1000)\n",
    "#\n",
    "def get_client_ids():\n",
    "    client_ids = np.array([],dtype='int')\n",
    "    for i, chunk in enumerate(train_reader):\n",
    "        client_ids = np.union1d(client_ids,chunk.ncodpers.unique())\n",
    "    return client_ids\n",
    "\n",
    "def create_client_ids_array():\n",
    "    client_ids = get_client_ids()\n",
    "    #client_ids.shape\n",
    "    np.save('client_ids',client_ids)\n",
    "    \n",
    "#create_client_ids_array()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class train_chunker(object):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self,chunksize):\n",
    "        self.chunksize     = chunksize\n",
    "        self.csv_path      = os.path.join(dir,'train_ver2.csv')\n",
    "        self.client_ids    = np.load('client_ids.npy')\n",
    "        self.client_number = self.client_ids.shape[0]\n",
    "\n",
    "        self.chunks = int(np.ceil(float(self.client_number)/self.chunksize))\n",
    "        print(str(self.chunks) + ' chunks')\n",
    "        self.reader = pd.read_csv(self.csv_path, chunksize=self.chunksize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 0 from 1365. (0.065743 secs)\n",
      "chunk 100 from 1365. (120.494977 secs)\n",
      "chunk 200 from 1365. (248.707856 secs)\n",
      "chunk 300 from 1365. (394.746841 secs)\n",
      "chunk 400 from 1365. (543.454813 secs)\n",
      "chunk 500 from 1365. (698.321827 secs)\n",
      "chunk 600 from 1365. (861.118067 secs)\n",
      "chunk 700 from 1365. (1025.78539 secs)\n",
      "chunk 800 from 1365. (1209.056001 secs)\n",
      "chunk 900 from 1365. (1400.426257 secs)\n",
      "chunk 1000 from 1365. (1596.30563 secs)\n",
      "chunk 1100 from 1365. (1790.651775 secs)\n",
      "chunk 1200 from 1365. (2007.253984 secs)\n",
      "chunk 1300 from 1365. (2227.411476 secs)\n"
     ]
    }
   ],
   "source": [
    "train_reader = get_train_reader()\n",
    "\n",
    "def find_client_index(ncodpers):\n",
    "    return np.argwhere(client_ids == ncodpers)\n",
    "\n",
    "def create_dataset_client_index_array():\n",
    "\n",
    "    train_data_client_indices = np.array([],dtype='int')\n",
    "    #train_data_client_indices = np.full((client_number),np.NaN,dtype='int')\n",
    "    j = 0\n",
    "\n",
    "    # measure process time\n",
    "    import time\n",
    "    t0 = time.clock()\n",
    "\n",
    "    for i, chunk in enumerate(train_reader):\n",
    "        if i%100==0:\n",
    "            proc_time = time.clock() - t0\n",
    "            print('chunk ' + str(i) + ' from ' + str(train_chunks) + '. (' + str(proc_time) + ' secs)')\n",
    "\n",
    "        chunk_size = len(chunk)\n",
    "        chunk_ncodpers = chunk.ncodpers\n",
    "        #print chunk_ncodpers[0]\n",
    "        stdout = Parallel(n_jobs=processors)\\\n",
    "        (delayed(find_client_index)(ncodpers) for ncodpers in chunk_ncodpers)\n",
    "\n",
    "        chunk_indices = np.full((chunk_size),np.NaN)\n",
    "        for i, indice in enumerate(stdout):\n",
    "            chunk_indices[i]= indice\n",
    "\n",
    "        train_data_client_indices = np.concatenate((train_data_client_indices, chunk_indices))\n",
    "        #    train_data_client_indices[i*chunksize:(i+1)*chunksize] = chunk_indices\n",
    "\n",
    "    np.save(csv_name + '__client_indices',train_data_client_indices)\n",
    "\n",
    "create_dataset_client_index_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client_ids[540143]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_reader = pd.read_csv(os.path.join('..',csv_name+'.csv'), chunksize=chunksize)\n",
    "\n",
    "for i, chunk in enumerate(train_reader):\n",
    "    column_ids = chunk.keys()\n",
    "#            print vars(this_chunk).iteritems()\n",
    "    product_ids = [s for s in column_ids if \"ult1\" in s]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 0 from 1365. (0.058221 secs)\n",
      "chunk 100 from 1365. (4.991478 secs)\n",
      "chunk 200 from 1365. (9.577015 secs)\n",
      "chunk 300 from 1365. (14.114101 secs)\n",
      "chunk 400 from 1365. (19.0129 secs)\n",
      "chunk 500 from 1365. (25.072869 secs)\n",
      "chunk 600 from 1365. (30.472365 secs)\n",
      "chunk 700 from 1365. (35.971098 secs)\n",
      "chunk 800 from 1365. (41.771981 secs)\n",
      "chunk 900 from 1365. (47.168134 secs)\n",
      "chunk 1000 from 1365. (52.131462 secs)\n",
      "chunk 1100 from 1365. (57.251175 secs)\n",
      "chunk 1200 from 1365. (62.122019 secs)\n",
      "chunk 1300 from 1365. (67.157229 secs)\n",
      "13647309\n"
     ]
    }
   ],
   "source": [
    "train_reader = pd.read_csv(os.path.join('..',csv_name+'.csv'), chunksize=chunksize)\n",
    "\n",
    "def create_utility_matrix():\n",
    "    product_number = 10\n",
    "\n",
    "    utility_matrix = np.zeros((client_number,product_number),dtype='int')\n",
    "    \n",
    "\n",
    "    # measure process time\n",
    "    t0 = time.clock()\n",
    "\n",
    "    j = 0\n",
    "    for i, chunk in enumerate(train_reader):\n",
    "        if i%100==0:\n",
    "            proc_time = time.clock() - t0\n",
    "            print('chunk ' + str(i) + ' from ' + str(train_chunks) + '. (' + str(proc_time) + ' secs)')\n",
    "\n",
    "        chunk_len = len(chunk)\n",
    "#        print len(chunk)\n",
    "#        print j+chunk_len\n",
    "        chunk_client_indices = client_index_array[j:j+chunk_len]\n",
    "        j+=chunk_len\n",
    "#        print chunk_client_indices\n",
    "    print j\n",
    "            \n",
    "create_utility_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_lines =  13647310\n",
      "(13650000,)\n"
     ]
    }
   ],
   "source": [
    "print 'train_lines = ',train_lines\n",
    "print client_index_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.array(stdout[:])[0]\n",
    "print stdout[0][0][0]\n",
    "print stdout[:][0][0]\n",
    "print stdout[0][:][0]\n",
    "print stdout[0][0][:]\n",
    "chunk_indices = np.full((chunksize),np.NaN)\n",
    "for i, indice in enumerate(stdout):\n",
    "    chunk_indices[i]= indice\n",
    "print chunk_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import necessary stuff\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_reader = pd.read_csv(train_csv, chunksize=chunksize)\n",
    "client_id = client_ids[0]\n",
    "client_matrix = pd.DataFrame()\n",
    "for i, chunk in enumerate(train_reader):\n",
    "    client_lines = chunk.ncodpers == client_id\n",
    "    #print client_lines == True\n",
    "    client_matrix = client_matrix.append(chunk.ix[client_lines])    \n",
    "    isinchunk = np.sum(client_lines)\n",
    "\n",
    "#    if isinchunk != 0.0:\n",
    "#        print i, isinchunk,len(client_matrix)\n",
    "#        #print chunk.ix[client_lines]\n",
    "#        print client_matrix\n",
    "#        raw_input()\n",
    "        #break\n",
    "client_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(train_reader):\n",
    "    pass\n",
    "print i\n",
    "#    print(chunk.ncodpers.unique().shape)\n",
    "\n",
    "#data=train_df.get_chunk()\n",
    "#data.ncodpers.unique()\n",
    "#for chunk_df in train_df:\n",
    "#    chunk_data = chunk_df.get_chunk()\n",
    "#    print chunk_data.shape\n",
    "#read\n",
    "#data.ncodpers.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import description table (describing the meaning of each column)\n",
    "description = read_csv(os.path.join(dir,'column_description.csv'),encoding = 'latin1')\n",
    "description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select product column names and build indeces range\n",
    "column_names = data.columns.values\n",
    "product_names = [(i, s) for i, s in enumerate(column_names) if 'ind_' in s and '_ult1' in s]\n",
    "product_indeces = [e[0] for e in product_names]\n",
    "mInd = min(product_indeces)\n",
    "MInd = max(product_indeces)\n",
    "product_indeces_range = range(mInd,MInd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "product_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a dataframe only with products\n",
    "product_df = data.iloc[:,product_indeces_range]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build a Series with number of products and use description as indeces\n",
    "desclist=description.iloc[product_indeces_range,1].tolist()\n",
    "num_products = Series(product_df.sum().tolist(),index=desclist)\n",
    "num_products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
